# -*- coding: utf-8 -*-
"""okay.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B-_bXVYdepvksFigGF2tcxFzeLxQBCPC
"""

from google.colab import files
files.upload()

from google.colab import drive
drive.mount('/content/drive')

import torch
print(f"CUDA Available: {torch.cuda.is_available()}")

# Run this cell first
!pip install -q transformers accelerate bitsandbytes trl datasets peft sentencepiece safetensors pyngrok streamlit

DATA_PATH = "code_optimization_dataset.csv"

# CONFIG
MODEL_NAME = "mistralai/Mistral-7B-v0.1"   # change if you want another Mistral variant
OUTPUT_DIR = "/content/drive/MyDrive/mistral-finetuned"
NGROK_AUTHTOKEN = "35t0D7y6l2yUBqAEjaI0nSNJVFk_67V8CLaVvQXr9ATPjpP5Y"  # <- replace this

import pandas as pd
from datasets import Dataset, DatasetDict
import random

df = pd.read_csv(DATA_PATH)
# Ensure required columns exist
required_cols = {"unoptimized_code", "optimized_code"}
assert required_cols.issubset(df.columns), f"Dataset must contain columns: {required_cols}"

def make_pair(row):
    meta = []
    if 'optimization_type' in row and pd.notna(row['optimization_type']):
        meta.append(f"Type: {row['optimization_type']}")
    if 'complexity_change' in row and pd.notna(row['complexity_change']):
        meta.append(f"Complexity: {row['complexity_change']}")
    meta_str = " | ".join(meta)
    instruction = f"Optimize the following Python function/code for performance and clarity.\n\n"
    if meta_str:
        instruction += f"# {meta_str}\n\n"
    instruction += "### Unoptimized code:\n" + row['unoptimized_code'].strip()
    response = row['optimized_code'].strip()
    return {"instruction": instruction, "response": response}

pairs = [make_pair(r) for _, r in df.iterrows()]
random.shuffle(pairs)

# Take only 3000 samples from your created pairs
pairs = pairs[:3000]

# Now split normally
split = int(len(pairs) * 0.9)
train_pairs = pairs[:split]
valid_pairs = pairs[split:]

train_ds = Dataset.from_list(train_pairs)
valid_ds = Dataset.from_list(valid_pairs)
dataset = DatasetDict({"train": train_ds, "validation": valid_ds})

print("Train size:", len(train_ds), "Validation size:", len(valid_ds))

def format_for_sft(example):
    # SFTTrainer expects a single text field per sample describing both instruction + response,
    # but we will pass dataset_text_field="text" and let trainer handle it.
    text = f"<s>[INST] {example['instruction']} [/INST] {example['response']}</s>"
    return {"text": text}

dataset = dataset.map(format_for_sft, remove_columns=dataset["train"].column_names)

pip install -U bitsandbytes

from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import BitsAndBytesConfig

compute_dtype = "bfloat16"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True
)

model = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-v0.1",
    quantization_config=bnb_config,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")
tokenizer.pad_token = tokenizer.eos_token

from peft import LoraConfig, get_peft_model, TaskType

lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM, # type of task to train on
    inference_mode=False, # set to False for training
    r=8, # dimension of the smaller matrices
    lora_alpha=32, # scaling factor
    lora_dropout=0.1 # dropout of LoRA layers
)

model = get_peft_model(model, lora_config)
print(model.print_trainable_parameters())

pip install trl

from trl import SFTTrainer
from transformers import TrainingArguments
import os
os.environ["WANDB_DISABLED"] = "true"
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=4,
    warmup_steps=5,
    num_train_epochs=1,
    learning_rate=2e-4,
    logging_steps=10,
    eval_strategy="steps",
    log_level="info",
    eval_steps=50,
    save_strategy="steps",
    save_steps=200,
    bf16=True,
)

trainer = SFTTrainer(
    model=model,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    args=training_args,
)

pip install --upgrade accelerate

trainer.train()
trainer.save_model(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)
print(f"Saved fine-tuned model to ./{OUTPUT_DIR}")

import torch
from transformers import pipeline
device = 0 if torch.cuda.is_available() else -1
pipe = pipeline("text-generation", model=OUTPUT_DIR, tokenizer=OUTPUT_DIR, device=device)

def infer_one(instr):
    prompt = f"<s>[INST] {instr} [/INST]"
    out = pipe(prompt, max_new_tokens=512, do_sample=False)[0]["generated_text"]
    # generator returns whole text; strip instruction part
    # The response may be concatenated after instruction, so attempt to remove prefix:
    prefix = f"<s>[INST] {instr} [/INST]"
    if out.startswith(prefix):
        return out[len(prefix):].strip()
    return out

for i in range(min(3, len(valid_pairs))):
    sample = valid_pairs[i]
    print("\n--- SAMPLE", i+1, "---")
    print("INSTRUCTION (truncated):\n", sample['instruction'][:400], "...\n")
    print("EXPECTED OPTIMIZED:\n", sample['response'][:400], "...\n")
    print("MODEL OUTPUT:\n", infer_one(sample['instruction'])[:800], "...\n")